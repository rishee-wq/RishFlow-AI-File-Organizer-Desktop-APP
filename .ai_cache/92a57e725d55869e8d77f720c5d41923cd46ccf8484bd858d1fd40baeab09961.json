{"mtime": 1763054533.9725032, "text": "Viva Questions \u2014 Real-Time Mask Detection (MobileNetV2 + OpenCV)\nA \u2014 Basic / high-level\n- 1. What problem does your project solve?\n- 2. Why is real-time mask detection important post-COVID?\n- 3. Give a high-level overview of the full pipeline (from camera \u2192 detection \u2192 inference \u2192 UI).\n- 4. Why did you choose MobileNetV2 as the backbone? What are its advantages for edge deployment?\n- 5. Why use OpenCV DNN SSD for face detection instead of running a full object detector like YOLO for faces?\nB \u2014 Dataset & preprocessing\n- 6. What datasets did you use (public + custom)? How did you combine / label them?\n- 7. How did you split data into train/validation/test sets? Why stratified split?\n- 8. What image preprocessing steps are required before feeding the network? (resize, color conversion,\npreprocess_input)\n- 9. Describe the data augmentation you used and why each augmentation helps.\n- 10. How did you handle class imbalance (if present)?\nC \u2014 Model architecture & training\n- 11. Describe the model architecture you built (MobileNetV2 base + head). What layers are in the head?\n- 12. Why do you freeze the base model initially? When & why would you unfreeze it?\n- 13. Which loss did you use and why (categorical_crossentropy vs binary_crossentropy)?\n- 14. What optimizer and learning rate did you choose and why?\n- 15. Which callbacks did you use (ModelCheckpoint, EarlyStopping, ReduceLROnPlateau) and what problems do\nthey solve?\n- 16. What are steps_per_epoch and validation_steps? How should they be set when you pass arrays vs generators?\n- 17. Explain batch size choice and effect on training stability and memory.\nD \u2014 Code specifics (training script)\n- 18. Walk me through how images are loaded and labels are encoded in your training script.\n- 19. Why use preprocess_input from MobileNetV2 before training/prediction?\n- 20. Why convert labels with LabelBinarizer() + to_categorical()?\n- 21. Explain the model.save() step and why you might also create a best-checkpoint file.\n- 22. How do you produce the training plot (what data does H.history contain)?\nE \u2014 Real-time app code (Tkinter + OpenCV)\n- 23. Explain how the face detector is loaded and used (prototxt + caffemodel + cv2.dnn.blobFromImage).\n- 24. Why is the video capture run in a separate thread? What would happen if you ran it on the main thread?\n- 25. How are faces preprocessed before calling maskNet.predict() in the GUI?\n- 26. Explain the prediction smoothing mechanism (prediction_cache + deque). What problem does it solve and what\nis a weakness of your approach?\n- 27. How can you improve tracking so the same person keeps an ID across frames (briefly describe SORT/centroid\ntracker)?\n- 28. How is the Matplotlib plot embedded and updated? How would you optimize frequent updates?\nF \u2014 Evaluation & the training graph (interpretation)\n- 29. Look at the training plot (loss & accuracy). What does it tell you about model behaviour during training?\n- 30. Why might training accuracy be very high from early epochs? (list possibilities)\n- 31. If validation loss is near zero and training loss also near zero, what checks would you perform to ensure results\nare valid?\n- 32. If your training loss decreases but validation loss increases (overfitting), what remedies would you apply?\nG \u2014 Deployment & optimization\n- 33. How would you convert/optimize your Keras model for Raspberry Pi? For Jetson?\n- 34. What is quantization? How does post-training integer quantization affect size/latency/accuracy?\n\n- 35. Explain the path Keras \u2192 TFLite and Keras \u2192 ONNX \u2192 TensorRT (high level).\n- 36. How would you measure throughput (FPS) on each target device? Which factors influence FPS?\nH \u2014 Federated learning & privacy\n- 37. Why consider federated learning for this project? What privacy benefits does it provide?\n- 38. Briefly explain the Federated Averaging algorithm (FedAvg).\n- 39. What are practical challenges of deploying federated learning across many edge devices? (non-IID data,\ncommunication, stragglers, security)\n- 40. How can you ensure privacy even when sending model updates? (secure aggregation, differential privacy)\nI \u2014 Robustness, limits & ethics\n- 41. What failure modes does your system have (lighting, occlusion, small faces, novel masks)? How can they be\nmitigated?\n- 42. Are there any ethical or privacy considerations to deploying this system in public spaces? How would you\naddress them?\n- 43. How would you measure fairness (across skin tones, ages, occlusions) and avoid biased errors?\nJ \u2014 Live demo & troubleshooting questions\n- 44. If I run the GUI and nothing shows on the canvas, what steps do you take to debug?\n- 45. If mask predictions flicker between frames for the same face, what is the likely cause and how to fix it?\n- 46. If the app is too slow on your device, what immediate changes would you try to speed inference?\n- 47. How do you handle multiple faces per frame? What changes if there are many (>20) faces?\nK \u2014 Deep-dive & extension questions (for examiners who probe)\n- 48. Explain MobileNetV2\u2019s inverted residual block and linear bottleneck and why it helps for lightweight models.\n- 49. Why use AveragePooling2D before Flatten in your head? What is the receptive-field effect?\n- 50. If you wanted to move from classification (face crop \u2192 mask/no mask) to a single-stage detector that outputs\nmask class and bounding box jointly, how would you design that (briefly mention YOLO-style or SSD\nmodification)?\n- 51. How would you retrain the model to include an 'incorrectly worn mask' class? What data and label strategies\nare needed?\n"}