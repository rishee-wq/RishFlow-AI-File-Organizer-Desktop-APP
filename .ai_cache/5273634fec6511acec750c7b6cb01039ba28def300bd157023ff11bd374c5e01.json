{"mtime": 1760894558.1141615, "text": "ScienceDirect\nAvailable online at www.sciencedirect.com\nProcedia Computer Science 231 (2024) 267\u2013274\n1877-0509 \u00a9 2024 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license ( https://creativecommons.org/licenses/by-nc-nd/4.0 )\nPeer-review under responsibility of the Conference Program Chairs\n10.1016/j.procs.2023.12.202\n10.1016/j.procs.2023.12.202 1877-0509\n\u00a9 2024 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\nPeer-review under responsibility of the Conference Program Chairs\n \nAvailable online at www.sciencedirect.com \nScienceDirect \nProcedia Computer Science 00 (2023) 000\u2013000  \nwww.elsevier.com/locate/procedia \n \n1877-0509 \u00a9 2023 The Authors. Published by Elsevier B.V. \nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) \nPeer-review under responsibility of the Conference Program Chairs.  \nThe 13th International Conference on Current and Future Trends of Information and \nCommunication Technologies in Healthcare (ICTH 2023)  \nNovember 7-9, 2023, Almaty, Kazakhstan \nA YOLOv5-Based Model for Real-Time Mask Detection in \nChallenging Environments \n  Saya Sapakovaa,*, Askar Sapakovb, Yelidana Yilibulec \na International Information Technology University, Manas str. 34/1, Almaty, 050000, Kazakhstan \nb Kazakh National Agrarian Research University, Valikhanov str. 137, Almaty, 050000, Kazakhstan  \ncInformation System Departement, Kazakh National University, Almaty, Kazakhstan   \nAbstract \nThis study aims to improve the YOLOV5  (You Only Look Once) model for real-time mask detection in complex scenarios, \nsuch as multiple objects, unclear features, and obstructions.  The experiment builds upon the initial model by refining its input \nstructure and incorporating the mosaic data augmentation technique. Additionally, it employs Spatial Pyramid Pooling Fusion \n(SPPF) to amalgamate local and global features at the featherMap level. The detection process is further enhanced by optimizi ng \nthe overlap computation between detection frames and target frames.  This refinement uses the Generalized Intersection over \nUnion (GioU) loss function to enhance target accuracy. The system is built in PyCharm, and the dataset consists of 6005 image s \nwith masked and unmasked faces for training.  The training set and test set maintain an 8:2 ratio.  The enhanced facial feature \nextraction network efficiently detects mask -wearers in real -time, maintaining high recognition rates in crowded public spaces, \nsupporting real-time virus transmission control in communal areas. \nThe enhanced model achieved an impressive 92.9% recognition accuracy in this experiment, surpassing other detection \nmodels, highlighting its high quality and effectiveness. \n \n\u00a9 2023 The Authors. Published by Elsevier B.V. \nThis is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) \nPeer-review under responsibility of the Conference Program Chairs. \nKeywords: YOLO; CNN; mask recognition; object detection; deep learning. \n \n \n* Corresponding author. Tel.: +7-707-279-77-11. \nE-mail address: s.sapakova@iitu.edu.kz;  \n \nAvailable online at www.sciencedirect.com \nScienceDirect \nProcedia Computer Science 00 (2023) 000\u2013000  \nwww.elsevier.com/locate/procedia \n \n1877-0509 \u00a9 2023 The Authors. Published by Elsevier B.V. \nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) \nPeer-review under responsibility of the Conference Program Chairs.  \nThe 13th International Conference on Current and Future Trends of Information and \nCommunication Technologies in Healthcare (ICTH 2023)  \nNovember 7-9, 2023, Almaty, Kazakhstan \nA YOLOv5-Based Model for Real-Time Mask Detection in \nChallenging Environments \n  Saya Sapakovaa,*, Askar Sapakovb, Yelidana Yilibulec \na International Information Technology University, Manas str. 34/1, Almaty, 050000, Kazakhstan \nb Kazakh National Agrarian Research University, Valikhanov str. 137, Almaty, 050000, Kazakhstan  \ncInformation System Departement, Kazakh National University, Almaty, Kazakhstan   \nAbstract \nThis study aims to improve the YOLOV5  (You Only Look Once) model for real-time mask detection in complex scenarios, \nsuch as multiple objects, unclear features, and obstructions.  The experiment builds upon the initial model by refining its input \nstructure and incorporating the mosaic data augmentation technique. Additionally, it employs Spatial Pyramid Pooling Fusion \n(SPPF) to amalgamate local and global features at the featherMap level. The detection process is further enhanced by optimizi ng \nthe overlap computation between detection frames and target frames.  This refinement uses the Generalized Intersection over \nUnion (GioU) loss function to enhance target accuracy. The system is built in PyCharm, and the dataset consists of 6005 image s \nwith masked and unmasked faces for training.  The training set and test set maintain an 8:2 ratio.  The enhanced facial feature \nextraction network efficiently detects mask -wearers in real -time, maintaining high recognition rates in crowded public spaces, \nsupporting real-time virus transmission control in communal areas. \nThe enhanced model achieved an impressive 92.9% recognition accuracy in this experiment, surpassing other detection \nmodels, highlighting its high quality and effectiveness. \n \n\u00a9 2023 The Authors. Published by Elsevier B.V. \nThis is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) \nPeer-review under responsibility of the Conference Program Chairs. \nKeywords: YOLO; CNN; mask recognition; object detection; deep learning. \n \n \n* Corresponding author. Tel.: +7-707-279-77-11. \nE-mail address: s.sapakova@iitu.edu.kz;  \n268 Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274\n2 Author name / Procedia Computer Science 00 (2023) 000\u2013000 \n1. Introduction \nIn recent times, numerous countries worldwide are confronting a fresh wave of epidemic repercussions. The \nmutation of disease strains has introduced further uncertainty to the already intricate landscape of epidemic \nprevention and control. Consequently, the act of wearing masks emerges as an exceedingly efficient and cost -\neffective approach to tackle this uncertainty during the normalization of epidemic control measures. Consequently, \nthe implementation of mask -wearing assessments in public spaces has evolved into a pivotal undertaking. Beyond \nthis context, donning masks in daily life serves to safeguard lives, curtail exposure to potential hazards, and enhance \nsafety and well -being in the face of respiratory infections or detrimental particulate emissions stemming from \nindustrial processes. \nThis underscores the significance of establishing an automated monitoring system for evaluating mask usage in \nroutine protective measures. Over the past couple of decades, the evolution of object detection can be broadly \ncategorized into two distinct phases: conventional target detection before 2014, and deep learning -driven target \ndetection thereafter [1]. The synergy of GPU technology and voluminous datasets has enabled deep learning to \nseamlessly undertake tasks previously reliant on multistage conventional machine learning approaches. This \ntransformative capability allows for end -to-end training through the direct input of substantial volumes of image, \nspeech, and text data [2 -3]. To democratize the functionality of supervising face mask compliance, the efficiency of \nthe mask detection network's model weight becomes a pivotal consideration. A succession of exemplary \nfoundational networks (such as VGGNet [4], ResNet [5], DenseNet [6], etc.) have been introduced. Yet, these \nnetworks often demand substantial computational resources and struggle to meet the real -time operational requisites \nof industrial applications. Thus emerged the concept of lightweight networks. SqueezeNet [7] pioneered this \napproach, beginning with structural optimizations followed by compression and expansion phases. Employing \nprevalent model compression techniques, SqueezeNet delivered performance akin to AlexNet [8], while utilizing a \nmere 1/50 of AlexNet's model parameters. Nevertheless, SqueezeNet continued to employ standard convolutional \ncomputations. Subsequently, MobileNet [9] harnessed the potency of deep separable convolutions to enhance \nnetwork speed, thereby propelling the utility of convolutional neural networks in mobile applications.  \nWith the rise of mobile devices and diverse application scenarios, lightweight networks have gained significance. \nSeveral experiments addressed face mask wear detection issues. In [11], a RetinaNet -based method achieved an \n86.45% AP on the validation set by transferring learning and using a pre -trained ResNet model. [12] introduced an \nenhanced spatial pyramid pooling structure with the YOLOv3 model, optimizing multiscale predictions and \nimproving accuracy by 14.9%. In [13], using the YOLOv5 model, the dataset was expanded to 30,000 images for \ntraining, achieving a final accuracy of 92.4% through flipping and rotating. \nIn this paper, an improved mask wearing detection method is proposed. In order to strike a balance between \nmodel accuracy and speed, a more lightweight improved YOLOv5 model is designed to compress the model and \nspeed up inference with little reduction in model accuracy, while reducing dependence on the hardware \nenvironment. To address the problem of unbalanced object scales in a picture caused by different distances from the \ncamera, YOLOv5 is used to perform multi -scale inspection of 20\u00d720, 40\u00d740, and 80\u00d780 to improve the detection \nperformance of the model for small objects.  \n2. REAL-TIME DETECTION MODEL FOR MASK WEARING BASED ON YOLOV5 \n2.1. YOLOv5 model \nOver the course of its development, YOLO (You Only Look Once), a general -purpose object detection model, \nhas undergone several iterations, including YOLOv1 [14], YOLOv2 [15], YOLOv3 [16], and YOLOv4 [17]. \nYOLOv1 employed a first-order structure to handle classification and target localization tasks. Subsequent versions, \nYOLOv2 and YOLOv3, focused on improving both speed and accuracy by reconfiguring the network structure. \nThese versions utilized different backbones, such as Darknet-53, ResNet-101, or ResNet-152, to achieve comparable \nlevels of accuracy while reducing computational complexity. This made object detection more feasible for industry \napplications. Finally, YOLOv4 achieved higher performance by combining several components, including a \nCSPDarknet53 backbone, an SPP add -on module, a path -aggregation neck, and a YOLOv3 (anchor -based) header, \nwhile still maintaining the ability to train on an ordinary GPU (1080Ti).  \n Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274 269 Author name / Procedia Computer Science 00 (2023) 000\u2013000  3 \nThe YOLO series of object detection models has undergone significant evolution since the development of \nYOLOv1, culminating in the creation of YOLOv5, which is more flexible than its predecessor, YOLOv4. The \nYOLOv5 project is currently maintained by Ultralytics Inc., and for the purposes of our experiments, we utilized the \nYOLOv5s model from the YOLOv5 model version 6.0, which features a network model structure consisting of four \nmain parts: Input, Backbone, Neck, and Prediction, as depicted in Figure 4. YOLOv5 provides four versions, namely \nYOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x, each of which offers increasing levels of accuracy and size. \nYOLOv5 uses channel and layer control factors similar to EfficientNet [18] to differentiate between versions based \non the bottleneck number. In practical applications, the most appropriate model size can be selected based on the \nspecific scenario in question. \n \nFig. 1. structure of YOLOv5 model. \nThe Input component of the YOLOv5 model comprises three key elements: image size scaling, adaptive \nanchor frame calculation, and Mosaic data augmentation. Typically, image datasets are resized to a uniform size \nprior to model training. However, the Mosaic data augmentation technique randomly selects four images that are \nresized, cropped, and arranged in a manner that stitches them together. After processing the label information, these \nimages are fed into the network for training, which has been shown to significantly improve the detection of small \ntargets. \nThe Backbone component of the YOLOv5 model comprises the Focus structure, SPP (Spatial Pyramid \nPooling) [19] structure, and CSP (Cross Stage Partial) structure [20]. This component extracts high, medium, and \nlow-level features from the input image. The SPP performs pooling on these features and concatenates the output \nlayer's features to generate a superimposed feature representation. The CSP structure incorporates two parameters, \ndepth_multiple and width_multiple, which enhances the flexibility of the overall Backbone design. \nThe Head component of the model mainly uses the FPN (Feature Pyramid Network) + PAN (Path Aggregation \nNetwork) combination for down sampling and up sampling. This facilitates feature fusion and improves prediction \nefficiency. \nThe Prediction component of the model is composed of three parts: classification loss, bounding box \nregression loss, and NMS (Non-Maximum Suppression). GIoU (Generalized Intersection over Union) loss is used to \nminimize the difference between the predicted and actual bounding boxes. It not only focuses on overlapping \nregions but also non -overlapping regions, providing a more accurate representation of overlap between the two. \nBinary cross-entropy loss is used to calculate classification loss. Finally, NMS is utilized to ensure that the candidate \nbox with the highest prediction probability is selected as the final prediction box. \n270 Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274\n4 Author name / Procedia Computer Science 00 (2023) 000\u2013000 \n2.2.  Improvements to the YOLOv5 Model \nTo effectively apply the YOLOv5 model to the task of large -scale mask detection, appropriate modifications and \ntraining are necessary. One key improvement is the use of the GIoU (Generalized Intersection over Union) loss \nfunction to calculate the bounding box regression loss[20], which can accelerate the model's convergence. GIoU loss \nnot only considers the overlapping region between predicted and ground truth bounding boxes but also takes into \naccount the areas outside the overlapping region. This approach can better reflect the degree of overlap between the \npredicted and ground truth bounding boxes and ensure that the predicted bounding box is as close as possible to the \ntrue bounding box, thereby improving the accuracy of mask detection. \n2.3. Mosaic Data Enhancement \nIn typical scenarios, images with varying sizes in the dataset are uniformly resized during both training and \ntesting before being fed into the network. However, this approach may not be optimal for detecting small targets. To \naddress this, this study proposes several methods, including Mosaic data augmentation and adaptive image scaling, \nfor enhancing the detection of small targets. \nThe Mosaic data augmentation method is implemented in the \"load_mosaic()\" function in the \"datasets.py\" file. \nThis method randomly selects four images and takes parts of them to form a mosaic image, as shown in the figure, \nwhere each color represents a different sample image. The parts outside the image boundary are discarded. This data \naugmentation method can effectively improve the detection of small targets and background details, enabling real -\ntime detection of solid targets with high efficiency. [2 1] The specific process of Mosaic data enhancement: in the \ncase of algorithmic processing, the images in the dataset are randomly stitched and a certain degree of the gray \nborder is added, and finally the images are scaled to a standard size and sent to the network for training. \n2.4. Bounding Box Regression loss function       \nThe field of target detection commonly utilizes various loss functions for target frame regression, including IoU \nloss, GIoU loss, DIoU loss (Distance IoU), CIoU loss [21] (Complete IoU), among others. The CIoU loss \ncalculation method takes into account the overlap area, centroid distance, and aspect ratio between the predicted \nframe and the real frame, making it theoretically the most effective. \nIntersection over Union [24](IoU) is a widely used metric in object detection that calculates the overlap between \nthe predicted and ground-truth bounding boxes. In anchor-based approaches, it is utilized not only for determining \npositive and negative samples but also for measuring the proximity of the predicted box to the ground-truth box, \nthanks to its scale-invariant nature refer to Eq. (1). \n  \n                                                         (1) \nGIoU [25] (Generalized Intersection over Union) incorporates additional penalties to the IoU metric to provide a \nmore precise representation of the interaction between the predicted and ground-truth bounding boxes refer to Eq. \n(2). \n                                                     (2) \nIn the equation, A is the predicted bounding box, B is the ground-truth bounding box, and C is their union. The \nGIoU loss is an extension of the IoU metric that introduces additional penalties to better capture the interaction \nbetween the predicted bounding box and the ground -truth bounding box. It takes into account not only the \noverlapping area of the two boxes but also the area of their union and the area of their non -overlapping regions. The \ncalculation of the GIoU loss is expressed in terms of the width and height of the minimum enclosing rectangle C  \nrefer to Eq. (3) , the center coordinates of the predicted and ground -truth boxes, and their respective width and \nheight. The GIoU loss has been found to produce better performance than the CIoU loss in experiments  refer to \nFig.5. \n Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274 271 Author name / Procedia Computer Science 00 (2023) 000\u2013000  5 \n             (3) \nThe diagram presented depicts a scenario where the predicted box A is displayed in blue, the ground truth box \nB is in red, and the yellow box C represents the smallest box that can enclose both A and B entirely refer to Fig.2. \n\u2022 Prediction box: A box computed from the model's target detection output. \n\u2022 Ground truth box: The manually annotated position stored in the annotation file. \n\u2022 Bounding box: Utilized to identify the object's position, with the common format consisting of the \ncoordinates of the top-left and bottom-right corners. \n \n \nFig. 2. Bonding regression loss of symbolic meaning. \n2.5.  Experiments and analysis of results \n2.5.1. Experimental environment and dataset \nIn this study, the RMFD (Real-World Masked Face Dataset) from the National Multimedia Software \nEngineering Technology Research Center of Wuhan University was utilized. A total of 6005 images were collected \nand divided into training and validation sets at a ratio of 8:2 for experimental and training purposes. The task of the \nface-mask detection system was to identify faces with masks and faces without masks. The system involved \nselecting a picture or real -time camera detection, outputting the entity detected as either a face or a mask, and \nproviding the corresponding detection data (0: face, 1: mask). The experimental process comprised of the following \nsteps: \n1. The dataset was imported into a computer, and the LabelImg software was employed to manually create a \nmarker box for the pictures. People wearing masks were labeled as a mask, while those without masks were labeled \nas a face. The pictures in the dataset could start training directly, and the corresponding XML files were generated. \n2. Images outside the dataset were converted into TXT files to produce the corresponding labels and target \nbox locations. \n3. The results were checked, and each generated TXT file had relevant information data, including 0 and 1 for \nno mask and mask, respectively. Other parameters represented the X and Y coordinate positions of the recognition \nframe. Prior to commencing the model learning training, relevant learning values were determined. The image size \nwas scaled to 640*640, batch size was set to 16, the epoch was set to 200, and the initial learning rate was set to \n0.01. \n2.5.2 Evaluation indicators \nThe YOLOv5 model undergoes an evaluation process that involves several performance metrics, including \nPrecision, Recall, Average Precision, Mean Average Pr ecision (mAP), and P -R curve . The assessment of model \nprecision primarily utilizes AP, mAP, and P -R curve, as they effectively capture the model's capacity to discern and \nacquire knowledge from images. \nThe Mean Average Precision (mAP) [20] serves as a crucial indicator of recognition accuracy. To plot the P -R \n272 Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274\n6 Author name / Procedia Computer Science 00 (2023) 000\u2013000 \ncurve, the maximum accuracy rate is calculated at varying recall rates, and the area enclosed by this curve reflects \nthe AP value for the respective category. In binary classification problems, a threshold is usually established, where \npredictions exceeding this threshold value are categorized as positive samples, while those falling below it are \nnegative samples. By assigning Recall to the horizontal axis and Precision to the vertical axis, a point is plotted on \nthe axis when setting a threshold, whereas multiple thresholds generate a curve known as the PR curve. \nWhen the objective is to predict the facial state with maximum accuracy, the primary focus is to enhance \nprecision. Conversely, when the objective is to predict the potential population of detection objects with or without \nobstructions as comprehensively as possible, the primary focus is to improve recall. Typically, an increase in the \nthreshold of binary classification leads to improved precision, whereas decreasing the threshold enhances recall. By \nanalyzing the PR curve, one can determine the optimal threshold. \nThe P-R curve for the enhanced YOLOv5 model after training on 6005 dataset images is depicted in Fig . 3. \nThe graph reveals that the average detection rate for faces is 0.965, while for masked faces, it is 0.894. The Mean \nAverage Precision value for the model is 0.929. These parameters reflect the experimental identification of \nunmasked faces over masked faces. In theory, an improvement in precision typically leads to a decrease in recall \nrate. Therefore, the experimental results suggest an improvement in detection outcomes. \n \n \nFig. 3. P-R curve and Mean Average Precision. \nFrames per second (FPS) is a measure of the number of frames transmitted per second. The human brain \nperceives a picture as continuous when the frame rate perceived by the naked eye is greater than 16 FPS, a \nphenomenon commonly re ferred to as visual transience . The algorithm in this paper has been verified to detect a \nmaximum of 30.0 FPS, owing to its unique structure. \n2.6.  Analysis of results \nAfter the model has been trained, it can be deployed in real-world settings by employing the user interface (UI) \nimage interface that is available in the PyCharm compiled environment. End -users have the flexibility to choose \nbetween manually selecting images for detection from a designated folder or utilizing a camera for real -time \ndetection. The model demonstrates a high level of proficiency in identifying face masks, and its output results \nprovide detailed labels indicating the presence of masks and predicted values pertaining to the states of faces with or \nwithout masks. Frames per second (FPS) is a measure of the number of frames transmitted per second. The human \nbrain perceives a picture as continuous when the frame rate perceived by the naked eye is greater than 16 FPS, a \nphenomenon commonly referred to as visual transience [ 21]. The algorithm in this paper has been verified to detect \na maximum of 30.0 FPS, owing to its unique structure. \n \n Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274 273 Author name / Procedia Computer Science 00 (2023) 000\u2013000  7 \n \n  Fig. 4. Detection result. \nThe YOLOv5 face detection model performs well in the case of multiple people in the image, with no missed \ndetections in multi -target detection. The experiment's results refer to Fig.4 demonstrate good performance, with \nvalues of approximately 0.9 or higher for specific facial features of objects, depending on the degree of blurring or \nthe position of the person. A mask determination status of 0.74 is given for the side face and incomplete facial \nfeatures. The model's mask recognition rate accurately determines whether an individual is wearing a mask or not, \nand the given mask or face state is primarily judged correctly. \n2.7. Comparison of algorithms \nThis research paper includes a comparative analysis of the YOLOv5 improved model against other popular \ntarget detection algorithms, such as SSD, Fast -R-CNN, and RetinaFace. The assembled algorithms were compared, \nand the results clearly indicate the scientific value and effectiveness of the proposed model refer to Table 1. \nThe results of the comparison are as follows: \n \nTable 1. Comparison of target detection algorithms \nDetection \nalgorithm \nmAP/% FPS Mask/% Face/% \nRetinaFace 83.97 43.92 76.52 87.36 \nSSD 79.23 15.6 77.26 81.20 \nFast-R-CNN 80.45 6.7 82.35 78.56 \nYOLOv5s 77.12 30 73.61 80.75 \nThis model 92.90 30 89.40 96.50 \n \nThe experimental results presented in this research paper demonstrate that the improved YOLOv5 face \ndetection algorithm is capable of accurately differentiating between faces with obstructions on the face and those \nwithout. The Mean Average Precision of the improved YOLOv5 model is significantly higher than that of other \ncommonly used target detection algorithms, which serves as evidence of the superiority of this model. \n3. CONCLUSION  \nIn summary, this research proposes a real -time mask detection algorithm based on YOLOv5 for complex scenes, \nwhich balances the trade -off between accuracy and speed. The algorithm demonstrates excellent performance in \nterms of accuracy, recall, and other evaluation metrics, meeting the real -time requirements of camera detection. \n274 Saya Sapakova  et al. / Procedia Computer Science 231 (2024) 267\u2013274\n8 Author name / Procedia Computer Science 00 (2023) 000\u2013000 \nNonetheless, the algorithm exhibits low detection rates in videos with a high number of targets and presents a \ncertain degree of missing detection in cases involving obscured, unclear, and small targets. Further research is \nneeded to investigate and improve the algorithm by combining mask-wearing features with other relevant features to \ndevelop a more applicable safety and health surveillance system that better aligns with the actual needs of society \nand daily life. \nReferences \n[1] Fu H, Li Y, Wang W,  Xu  X. (2022). Target detection models based on Deep Learning. International Conference on Cloud Computing, \nPerformance Computing, and Deep Learning (CCPCDL 2022). https://doi.org/10.1117/12.2641024  \n[2] Poostchi M, Silamut K, Maude R, Jaeger S, Thoma G. (2018). Image analysis and machine learning for detecting malaria. Transla tional \nResearch, 194:36\u201355. https://doi.org/10.1016/j.trsl.2017.12.004 \n[3] Jun H, Shuai L, Jinming S, Yue L, Jingwei W, Peng J. (2018). Facial Expression Recognition Based on VGGNet Convolutional Neur al \nNetwork. Chinese Automation Congress. https://doi.org/10.1109/cac.2018.8623238 \n[4] Wu Z, Shen C, Van Den Hengel A. (2019). Wider or Deeper: Revisiting the ResNet Model for Visual Recognition. Pattern Recognit ion, \n90:119\u2013133. https://doi.org/10.1016/j.patcog.2019.01.006 \n[5] Huang G, Liu S, Van Der Maaten L, Weinberger K. (2018). CondenseNet: An Efficient DenseNet  Using Learned Group Convolutions. \nComputer Vision and Pattern Recognition. https://doi.org/10.1109/cvpr.2018.00291 \n[6] Ucar F, Korkmaz D. (2020). COVIDiagnosis -Net: Deep Bayes -SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID -19) \nfrom X-ray images. Medical Hypotheses, 140:109761. https://doi.org/10.1016/j.mehy.2020.109761 \n[7] Deep Convolutional Neural Networks (AlexNet) \u2014 Dive into Deep Learning 1.0.0 -beta0 documentation. Available: \nhttps://d2l.ai/chapter_convolutional-modern/alexnet.html \n[8] Howard A, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. (2017). MobileNets: Efficient Convolutional \nNeural Networks for Mobile Vision Applications. ArXiv (Cornell University). http://export.arxiv.org/pdf/1704.04861 \n[9] Ma N, Zhang X, Zheng H, Sun J. (2018). ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design. Springer EB ooks, 122\u2013\n138. https://doi.org/10.1007/978-3-030-01264-9_8 \n[10] DENG H X. (2020) Method of mask wearing detection based on transfer learning and RetinaNet. J. Electronic Technology & Softwa re \nEngineering, 5:209-211.\uff08in Chinese\uff09 \n[11] Sik-Ho Tsang. (2018). Review: YOLOv1\u200a\u2014 \u200aYou Only Look Once (Object Detection). Retrieved From the Towards Data Science. Available: \nhttps://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89. \n[12] Sharma A. (2022). A Better, Faster, and Stronger Object Detector (YOLOv2) - PyImageSearch. Retrieved From the Py Image Search. \nAvailable: https://pyimagesearch.com/2022/04/18/a-better-faster-and-stronger-object-detector-yolov2/. \n[13] Redmon  J. (2018). YOLOv3: An Incremental Improvement. Retrieved From the Papers With Code. Available: \nhttps://paperswithcode.com/method/yolov3. \n[14] Bochkovskiy A.(2020). YOLOv4: Optimal Speed and Accuracy of Object Detection. Retrieved From the Papers With Code. Available:  \nhttps://paperswithcode.com/method/yolov4. \n[15] JiangWen. (2022). Study on Resnet and EfficientNet Remote Sensing Image Scene Classification. Computer Science and Applicatio n, \n12(05):1301\u20131313. doi: 10.12677/CSA.2022.125130 \n[16] Shi L, Zhou Z, Guo Z.(2021). Face anti-spoofing using Spatial Pyramid Pooling. 2020 25th International Conference on Pattern Recognition \n(ICPR) (pp. 2126-2133). IEEE. \n[17] Brailsford S, Potts C, Smith B. (1998). Constraint satisfaction problems: Algorithms and applications. Department of Accounti ng and \nManagement Science, University of Southampton. \n[18] Wang X, Song J. (2021). ICIoU: Improved Loss Based on Complete Intersection Over Union for Bounding Box Regression. IEEE Acce ss, \n9:105686\u2013105695. doi: 10.1109/ACCESS.2021.3100414 \n[19] Redmon J, Divvala S, Girshick R, Farhadi A. (2016). You Only Look Once: Unified, Real -Time Object Detection. 2016 IEEE Conference \non Computer Vision and Pattern Recognition (CVPR) (pp. 779-788). IEEE. \n[20] Zhang G, Du Z, Lu W. (2022). Dense Pedestrian Detection Based on YOLO -V4 Network Reconstruction and CIoU Loss Optimization. \nJournal of Physics: Conference Series, 2171(1):012019. doi: 10.1088/1742-6596/2171/1/012019  \n[21] Pereira N. (2022). PereiraASLNet: ASL letter recognition with Yolox taking mean average precision and inference time consider ations. \n2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP) (pp. 1-6). IEEE. \n \n \n \n "}