{"mtime": 1768898866.6146092, "text": " \n205   \n 1. Artificial Intelligence and Machine Learning Track                         Course Code Course name L T P C CSAI2017P Applied Machine Learning 4 0 1 5 Total Units to be Covered: 07 Total Contact Hours:90 Prerequisite(s): Elements of AIML-  CSAI2018 Syllabus version: 1.0  Course Objectives 1. Understand the core concepts and techniques of machine learning and artificial intelligence. 2. Develop machine learning models using popular libraries and frameworks. 3. Evaluate the performance of machine learning models using appropriate metrics. 4. Apply machine learning to various real-world problems and domains. Course Outcomes On completion of this course, the students will be able to  1. Recall and define key machine learning concepts, terminologies, and algorithms. 2. Describe the differences between supervised, unsupervised, and reinforcement learning. 3. Apply data preprocessing techniques to clean, transform, and prepare datasets for machine learning. 4. Apply, compare, and contrast the strengths and weaknesses of different machine learning algorithms.  CO-PO Mapping  Program Outcomes Course Outcomes PO1 PO2 PO3 PO4 PO5 PO6 PO7 PO8 PO9 PO10 PO11 PO12 PSO1 PSO2 PSO3 CO 1 1 1 1 - - - - - - - - - 1 - 3 CO 2 1 1 1 - - - - - - - - - 1 - 3 CO 3 1 1 1 - - - - - - - - - 1 - 3 CO4 1 1 1 - - - - - - - - - 2 - 3 \n \n206  \n \nAverage 1 1 1 - - - - - - - - - 1.25 - 3 \n \n \n1 \u2013 Weakly Mapped (Low)  2 \u2013 Moderately Mapped (Medium) \n \n3 \u2013 Strongly Mapped (High)                  \u201c_\u201d means there is no correlation \n \n \n \n \nSyllabus \n \n \nUnit I: Introduction                 3 Lecture Hours \nOverview of machine learning and its applications, Types of machine learning: \nsupervised, unsupervised, reinforcement, Python and libraries for machine learning \n(e.g., NumPy, Pandas, scikit-learn)                                 \nUnit II: Loss functions                             3 Lecture Hours \nMean Squared Error (MSE), Mean Absolute Error (MAE), Huber Loss, Binary Cross-\nEntropy Loss (Log Loss), Categorical Cross-Entropy Loss, Sparse Categorical Cross-\nEntropy Loss, Hinge Loss (SVM Loss), Triplet Loss                   \n \nUnit III: Optimizer function                       6 Lecture Hours \nStochastic gradient descent, Mini-Batch Gradient Descent, Momentum, Adaptive \ngradient algorithm (Adagrad), Adam (Adaptive Moment Estimation), RMSprop (Root \nMean Square Propagation), Adadelta \n \nUnit IV: Data Preprocessing                      10 Lecture Hours \nData Cleaning: handling Missing Data, Handling Outlier, Data Transformation: Feature \nScaling, Feature Encoding, Feature Engineering, Data Reduction: Dimensionality \nreduction technique, feature selection, Data Splitting: Cross validation techniques, \nHandling imbalanced data: Oversampling techniques, under sampling techniques. \n \nUnit V: Regression                 12 Lecture Hours \nIntroduction to Regression, Regression examples, Regression models, Steps in \nregression analysis, Linear regression, Simple linear regression, Mathematical proof \nof linear regression, Least squares estimation, Least squares regression-Line of best \nfit, Illustration, Direct regression method, Maximum likelihood estimation, Coefficient \nof determination (R-squared), Checking model adequacy, Over-fitting, Detecting over-\nfit models: Cross validation, Logistic regression, Mathematical proof of logistic \n \n207  \n \nregression, multiple linear regression, Multiple linear regression model building, \nMathematical proof of Multiple linear regression model, Interpretation of multiple linear \nregression coefficients-Partial regression coefficients, Standardized regression \ncoefficients, Missing data, Validation of multiple regression model, regularization, \nridge and lasso regularization. \n \nUnit VI: Classification                14 Lecture Hours \nIntroduction, ML classifier, Classification and general approach, Classification \nalgorithms, Instance based learning, K-Nearest neighbour, Decision trees, Attribute \nselection measure: Information gain, ID3 algorithm, Converting a tree to rules, \nBayesian algorithms, Ensemble, Ensemble of classifiers, Bagging, Boosting, Random \nforests, Neural networks, Activation functions, Feedforward neural network, Multi-layer \nperceptron, Back propagation algorithm, Recurrent or feedback architecture, \nPerceptron rule, Multilayer networks and back propagation algorithm, Support vector \nmachine, Classification model evaluation and selection, ROC curves, AUC curves.    \n \nUnit VII: Clustering Techniques             12 Lecture Hours \nIntroduction to Clustering, Clustering algorithms, Statistics associated with cluster \nanalysis, General applications of clustering, Clustering as a pre-processing tool, \nSimilarity and dissimilarity between objects, Type of data in clustering analysis, Binary \nvariables, Nominal variables, Ordinal variables, Cluster centroid and distances, \nHierarchical clustering, Hierarchical Agglomerative Clustering (HAC), Hierarchical \nAgglomerative Clustering: Linkage method, Hierarchical Agglomerative Clustering: \nVariance and Centroid method, Cluster distance measures, agglomerative clustering, \nDistance between two clusters, Hierarchical clustering: Time and Space requirements, \nK - means clustering, The K-medoids clustering method, CLARA (Clustering Large \nApplications), Density based clustering methods, DBSCAN.           \n \n              \n \nTotal lecture Hours 60 \nReferences* \nTextbooks 1. Introduction to Machine Learning with Python\" by \nAndreas C. M\u00fcller and Sarah Guido \n2.  Pattern Recognition and Machine Learning\" by \nChristopher M. Bishop \nReference books 1.  Machine Learning: A Probabilistic Perspective\" by Kevin \nP. Murphy \n2.  Python Machine Learning\" by Sebastian Raschka and \nVahid Mirjalili \n \n \n208  \n \nModes of Evaluation: Quiz/Assignment/ presentation/ extempore/ Written \nExamination etc. \n \nExamination Scheme \nComponents IA MID SEM End Sem Total \nWeightage (%) 50  20 30 100 \n \n \n   \n \n  \n \n209  \n \nApplied Machine Learning Lab \n \nList of Experiments \nBelow is a list of small machine learning-based projects suitable for this lab work. \nThrough these projects students are expected to implement the concepts of data \npreprocessing and machine learning algorithms. These projects cover various \nmachine learning techniques and can serve as valuable learning experiences: \n \nExperiment 1 Predicting Housing Prices: Develop a regression model to predict \nhouse prices based on features like location, size, and amenities. \nExperiment 2 Iris Flower Classification: Use the Iris dataset to build a \nclassification model that predicts the species of iris flowers. \nExperiment 3 Handwritten Digit Recognition: Implement a digit recognition \nsystem using the MNIST dataset and a neural network. \nExperiment 4 Breast Cancer Diagnosis: Develop a breast cancer classification \nmodel using medical imaging data (e.g., mammograms). \nExperiment 5 Sentiment Analysis: Create a sentiment analysis tool that \nclassifies text reviews as positive or negative using natural language processing \n(NLP) techniques. \nExperiment 6 Spam Email Detection: Build a spam email filter using text \nclassification algorithms. \nExperiment 7 Predicting Stock Prices: Develop a time series prediction model \nto forecast stock prices. \nExperiment 8 Credit Risk Assessment: Build a credit scoring model to assess \nthe creditworthiness of applicants using historical financial data. \nExperiment 9 Recommendation System: Create a movie or book \nrecommendation system based on user behavior data (collaborative or content-\nbased). \nExperiment 10 Anomaly Detection: Implement an anomaly detection system for \ndetecting outliers in data (e.g., fraud detection). \nExperiment 11 Customer Churn Prediction: Develop a model to predict customer \nchurn in a subscription-based business. \nExperiment 12 Fake News Detection: Create a model to classify news articles as \nreal or fake based on their content. \n \n210  \n \nExperiment 13 Disease Diagnosis from Medical Images: Use medical imaging \ndata (e.g., X-rays) to diagnose diseases or conditions. \nExperiment 14 Traffic Sign Recognition: Build a model that can recognize and \nclassify traffic signs in images or video streams. \n \nTotal Lab hours 30 \nReferences* \nTextbooks 1. Andreas C. M\u00fcller and Sarah Guido, \n\"Introduction to Machine Learning with \nPython\", Shroff/O'Reilly, 2016. \n2. Christopher M. Bishop , \"Pattern \nRecognition and Machine Learning\", \nSpringer, 2016. \nReference books 1. Kevin P. Murphy, \"Machine Learning: A \nProbabilistic Perspective\", MIT Press, 2012. \n2. Sebastian Raschka and Vahid Mirjalili, \n\"Python Machine Learning\", 2nd Edition, \nPackt Publishing, 2017. \n \n \nModes of Evaluation: Quiz/Assignment/ presentation/ extempore/ Written \nExamination etc. \n \nExamination Scheme: Continuous Assessment \nComponents  Quiz & Viva  Performance & Lab Report  \nWeightage (%)  50 %  50 %  \n \n \n  "}